# -*- coding: utf-8 -*-
"""Senior AI Engineer Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12ijV8QvXrRp1GhGwZ612uWrla9CdQZKo
"""

!pip install google-play-scraper sentence-transformers pandas numpy scikit-learn tqdm

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from google_play_scraper import reviews
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

def fetch_reviews(app_id, days=30):
    data = []
    cutoff = datetime.now() - timedelta(days=days)

    result, _ = reviews(
        app_id,
        lang="en",
        country="in",
        count=1200
    )

    for r in result:
        if r["at"] >= cutoff:
            data.append({
                "date": r["at"].date(),
                "content": r["content"]
            })

    return pd.DataFrame(data)

df_reviews = fetch_reviews("in.swiggy.android")
df_reviews.head()

daily_batches = {
    d: g["content"].tolist()
    for d, g in df_reviews.groupby("date")
}

print("Total days with data:", len(daily_batches))

model = SentenceTransformer("all-MiniLM-L6-v2")

def classify_requests(reviews):
    requests = []

    request_keywords = [
        "add", "bring back", "please include", "need", "want",
        "should have", "feature", "option", "support", "allow"
    ]

    for text in reviews:
        text_l = text.lower()
        if any(k in text_l for k in request_keywords):
            requests.append(text_l)

    return requests

def normalize_requests(requests, threshold=0.72):
    if not requests:
        return []

    embeddings = model.encode(requests)
    used = set()
    normalized = []

    for i, emb in enumerate(embeddings):
        if i in used:
            continue

        sims = cosine_similarity([emb], embeddings)[0]
        group_idx = [j for j, s in enumerate(sims) if s > threshold]

        used.update(group_idx)
        normalized.append(requests[i])

    return normalized

def build_request_trends(daily_batches):
    trends = {}

    for date, reviews in tqdm(daily_batches.items()):
        reqs = classify_requests(reviews)
        merged = normalize_requests(reqs)

        for r in merged:
            if r not in trends:
                trends[r] = {}
            trends[r][date] = trends[r].get(date, 0) + 1

    return trends

request_trends = build_request_trends(daily_batches)

end_date = max(daily_batches.keys())
dates = [end_date - timedelta(days=i) for i in range(29, -1, -1)]

rows = []
for topic, counts in request_trends.items():
    row = {"Topic": topic[:60]}
    for d in dates:
        row[str(d)] = counts.get(d, 0)
    rows.append(row)

trend_df = pd.DataFrame(rows)
trend_df

trend_df.to_csv("trend_report.csv", index=False)
print("trend_report.csv saved")

from google.colab import files
files.download("trend_report.csv")

